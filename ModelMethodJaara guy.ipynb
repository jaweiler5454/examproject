{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import random, math\n",
    "import numpy as np\n",
    "import gym\n",
    "import utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras-related libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the penalty from estimation using Huber loss function\n",
    "def hubert_loss(y_true, y_pred):    # sqrt(1+a^2)-1 \n",
    "    err = y_pred - y_true\n",
    "    return K.mean( K.sqrt(1+K.square(err))-1, axis=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, stateCnt, actionCnt): \n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.model = self._createModel()\n",
    "        self.model_ = self._createModel()  # target network\n",
    "\n",
    "        # self.model.load_weights(\"mc.h5\")\n",
    "\n",
    "    def _createModel(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(output_dim=128, activation='relu', input_dim=stateCnt))\n",
    "        model.add(Dense(output_dim=128, activation='relu'))\n",
    "\n",
    "        model.add(Dense(output_dim=actionCnt, activation='linear'))\n",
    "\n",
    "        opt = RMSprop(lr=0.0001)\n",
    "        # opt = optimizers.Adadelta()\n",
    "\n",
    "        model.compile(loss=hubert_loss, optimizer=opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def updateTargetModel(self):\n",
    "        self.model_.set_weights(self.model.get_weights())\n",
    "\n",
    "    def train(self, x, y, w=None, epoch=1, verbose=0):\n",
    "        self.model.fit(x, y, sample_weight=w, batch_size=32, nb_epoch=epoch, verbose=verbose)\n",
    "\n",
    "    def predict(self, s, target=False):\n",
    "        if target:\n",
    "            return self.model_.predict(s)\n",
    "        else:\n",
    "            return self.model.predict(s)\n",
    "\n",
    "    def predictOne(self, s, target=False):\n",
    "        return self.predict(s.reshape(1, self.stateCnt), target).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- MEMORY --------------------------\n",
    "class Memory:   # stored as ( s, a, r, s_ )\n",
    "    samples = []\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.samples.append(sample)        \n",
    "\n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- AGENT ---------------------------\n",
    "MEMORY_CAPACITY = 100000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "MAX_EPSILON = 0.8\n",
    "MIN_EPSILON = 0.1\n",
    "LAMBDA = 0.001      # speed of decay\n",
    "\n",
    "UPDATE_TARGET_FREQUENCY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.brain = Brain(stateCnt, actionCnt)\n",
    "        self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        self.memory.add(sample)        \n",
    "\n",
    "    def act(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.actionCnt-1)\n",
    "        else:\n",
    "            return np.argmax(self.brain.predictOne(s))\n",
    "\n",
    "    def replay(self):\n",
    "        ##----- debug\n",
    "        if self.steps % 1000 == 0:\n",
    "            P = [\n",
    "                [ 0.874334,  0.703311], # s__ -> exit\n",
    "                [ 0.819632,  0.69813 ], # s_ -> s__\n",
    "                [ 0.765333,  0.697897], # s -> s_\n",
    "                [ 0.716243,  0.109933], # s1 \n",
    "                [ 0.724484,  0.10595 ], # s0 -> s1\n",
    "            ]\n",
    "\n",
    "            pred = self.brain.predict( np.array(P) )\n",
    "\n",
    "            for o in pred:\n",
    "                sys.stdout.write(str(o[0]) + \" \" + str(o[1])+\" \")\n",
    "\n",
    "            print(\";\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        if self.steps % 50000 == 0:\n",
    "            utils.displayBrain(self.brain, res=50)\n",
    "            utils.printFPS(self.steps)\n",
    "\n",
    "        #~~~~~~ debug\n",
    "\n",
    "        if self.steps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            self.brain.updateTargetModel()\n",
    "\n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        batchLen = len(batch)\n",
    "\n",
    "        states = np.array([ o[0] for o in batch ])\n",
    "        states_ = np.array([ ([0,0] if o[3] is None else o[3]) for o in batch ])\n",
    "\n",
    "        p = agent.brain.predict(states)\n",
    "        p_ = agent.brain.predict(states_, target=True)\n",
    "\n",
    "        x = np.zeros((batchLen, self.stateCnt))\n",
    "        y = np.zeros((batchLen, self.actionCnt))\n",
    "        \n",
    "        for i in range(batchLen):\n",
    "            o = batch[i]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * np.amax(p_[i])\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t            \n",
    "\n",
    "        self.brain.train(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- ENVIRONMENT ---------------------\n",
    "class Environment:\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.env = gym.make(problem)\n",
    "\n",
    "        high = self.env.observation_space.high\n",
    "        low = self.env.observation_space.low\n",
    "\n",
    "        self.mean = (high + low) / 2\n",
    "        self.spread = abs(high - low) / 2\n",
    "\n",
    "    def normalize(self, s):\n",
    "        return (s - self.mean) / self.spread\n",
    "\n",
    "    def run(self, agent):\n",
    "        s = self.normalize(self.env.reset())\n",
    "        R = 0 \n",
    "\n",
    "        while True:            \n",
    "            # self.env.render()\n",
    "\n",
    "            a = agent.act(s)\n",
    "\n",
    "            # map actions; 0 = left, 2 = right\n",
    "            if a == 0: \n",
    "                a_ = 0\n",
    "            elif a == 1: \n",
    "                a_ = 2\n",
    "\n",
    "            s_, r, done, info = self.env.step(a_)\n",
    "            s_ = self.normalize(s_)\n",
    "\n",
    "            if done: # terminal state\n",
    "                s_ = None\n",
    "\n",
    "            agent.observe( (s, a, r, s_) )\n",
    "            s = s_\n",
    "            R += r\n",
    "\n",
    "            agent.replay()            \n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        utils.eprint(\"Total reward:\", R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-11 13:31:34,572] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0386273 -0.144465 0.0373364 -0.137642 0.0365026 -0.132556 0.0112535 -0.109463 0.0112951 -0.111185 ;\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'displayBrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5d8a49f9b7a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mc.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-60cc9b02c169>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mR\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-affb6b411413>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayBrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintFPS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'displayBrain'"
     ]
    }
   ],
   "source": [
    "#-------------------- MAIN ----------------------------\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "PROBLEM = 'MountainCar-v0'\n",
    "env = Environment(PROBLEM)\n",
    "\n",
    "stateCnt  = env.env.observation_space.shape[0]\n",
    "actionCnt = 2 #env.env.action_space.n\n",
    "\n",
    "agent = Agent(stateCnt, actionCnt)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        env.run(agent)\n",
    "finally:\n",
    "    agent.brain.model.save(\"mc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
